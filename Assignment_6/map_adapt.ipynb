{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all the necessary lib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import os\n",
    "from numpy.linalg import norm\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def MAP_Adaptation(speaker_data_path):\n",
    "    \"\"\"Doing one round of EM Algorithm for Adapting the Speaker Data \"\"\"\n",
    "    speaker_data = pd.read_csv(speaker_data_path,sep=' ',header=None)\n",
    "    # speaker_data = speaker_data.drop(columns=0,axis=1)\n",
    "\n",
    "\n",
    "    ubm_model = pickle.load(open(f'gmm_speaker_ubm_{cluster_Size}_zmuv.pkl' ,'rb'))\n",
    "    N,D = speaker_data.shape\n",
    "    # print('N',N , 'D',D)\n",
    "    K = int(args.K)\n",
    "    # print(K)\n",
    "    w_ubm = ubm_model[0]\n",
    "    mu_ubm = ubm_model[1]\n",
    "    sigma_ubm = ubm_model[2]\n",
    "\n",
    "    \"\"\"For Now Writing the E_Step Algorithm Code here and then changing the Code Structure of K-Means\"\"\"\n",
    "    gamma_nk = np.zeros((N,K))\n",
    "    for i in range(0,K):\n",
    "    \n",
    "        # gamma_nk[:,i] = np.log(w_ubm[i]) * mp.logpdf(speaker_data ,mu_ubm[i],sigma_ubm[i,:,:]).ravel()\n",
    "        gamma_nk[:,i] = (w_ubm[i]) * mp.pdf(speaker_data ,mu_ubm[i],sigma_ubm[i,:,:]).ravel()\n",
    "\n",
    "\n",
    "    N_k = np.sum(gamma_nk,axis=0)\n",
    "    gamma_nk /= N_k\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    mu_updated = []\n",
    "    for i in range(K):\n",
    "        mu_updated.append(np.dot(gamma_nk[:,i],speaker_data))   \n",
    "    mu_upd = np.stack(mu_updated,axis=0)\n",
    "    # print('mu_up',mu_upd.shape)\n",
    "    \n",
    "    r =16\n",
    "    alpha_k = N_k/(N_k + r)\n",
    "    # print('alpha_k_1',alpha_k.shape)\n",
    "\n",
    "    alpha_k = alpha_k.reshape(-1, 1)\n",
    "\n",
    "    mu_new = alpha_k*mu_upd+(1-alpha_k)*mu_ubm\n",
    "\n",
    "    \"\"\"It came to my notice that the mu values are not adapting\n",
    "    Possibilities are:\n",
    "                    1.alpha value is small and (1- alpha)value is large so the mu_new = mu_ubm   --->took ZMUV\n",
    "                    2.Why is the alpha value small-->N-K-->gamma-->ubm-->all program is wrong :)\n",
    "                    3.Gamma value was low in E_Step and M_step , used that value to calculate parameters\n",
    "                    \n",
    "            Fix     Took Zero Mean Unit Variance to bring the MFCC Feature Vectors and fixed all the problems \n",
    "                        including the Accuracy   \"\"\"\n",
    "    \n",
    "\n",
    "        \n",
    "\n",
    "    return mu_new , sigma_ubm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"RUN This ONCE\n",
    "\n",
    "This combines all the train dataset into one single file for MAP Adaptation\"\"\"\n",
    "\n",
    "train_dir_path = '/home/tenet/Documents/IITM/CS6300_Speech_Technology/CS22Z121/Assignment_6/mfcc_zeromean_unit_variance/mfcc_13_split_train'\n",
    "adapted_train_speaker_dir = 'speaker_adapted_ubm_train_1024_zmuv'\n",
    "# if not os.path.exists(adapted_train_speaker_dir):\n",
    "#     os.makedirs(adapted_train_speaker_dir)\n",
    "\n",
    "for train_folders in os.listdir(train_dir_path):\n",
    "    comb = []\n",
    "    if not os.path.exists(os.path.join(adapted_train_speaker_dir,train_folders)):  \n",
    "        os.makedirs(os.path.join(adapted_train_speaker_dir,train_folders) )\n",
    "    for train_files in os.listdir(os.path.join(train_dir_path,train_folders)):\n",
    "        file_path = os.path.join(train_dir_path,train_folders,train_files)\n",
    "        file_read = pd.read_csv(file_path,sep=' ',header=None)\n",
    "        comb.append(file_read)\n",
    "        # file_prefix = pathlib.Path(train_folders)\n",
    "        # print(file_prefix)\n",
    "    # print(np.array(comb).shape)    \n",
    "    Y = pd.concat(comb)\n",
    "    # Y.drop(columns=0,axis=1)\n",
    "\n",
    "    # print(Y.shape)\n",
    "    # # save_file_path = os.path.join(adapted_train_speaker_dir,train_folders)\n",
    "    # # print(save_file_path)\n",
    "    # filename = \"{}.mfcc\".format(train_folders)\n",
    "    # Y.to_csv(filename,sep=' ',header=None)  \n",
    "    output_filename = \"{}.mfcc\".format(train_folders)\n",
    "    output_filepath = os.path.join(adapted_train_speaker_dir,train_folders,output_filename)\n",
    "    np.savetxt(output_filepath,Y,delimiter=' ')      \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir_path = '/home/tenet/Documents/IITM/CS6300_Speech_Technology/CS22Z121/Assignment_6/mfcc_with_cepstral_subtraction/mfcc_train'\n",
    "\n",
    "adapted_train_speaker_dir = 'speaker_adapted_ubm_train_2048_zmuv_3itr'\n",
    "if not os.path.exists(adapted_train_speaker_dir):\n",
    "    os.makedirs(adapted_train_speaker_dir)\n",
    "c = 0    \n",
    "for train_files in sorted(os.listdir(train_dir_path)):\n",
    "\n",
    "    speakerpath = os.path.join(train_dir_path,train_files)\n",
    "    \n",
    "    o_filename = \"{}.pkl\".format(train_files.split('.')[0])\n",
    "    \n",
    "    \n",
    "    pickle.dump(MAP_Adaptation(speakerpath),open(os.path.join(adapted_train_speaker_dir,o_filename),'wb'))   \n",
    "    c +=1\n",
    "    print('itr:',c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dir_path = '/home/tenet/Documents/IITM/CS6300_Speech_Technology/CS22Z121/Assignment_6/mfcc_with_cepstral_subtraction/mfcc_13_split_test'\n",
    "\n",
    "adapted_test_speaker_dir = 'speaker_adapted_ubm_test_2048_zmuv_3itr'\n",
    "if not os.path.exists(adapted_test_speaker_dir):\n",
    "    os.makedirs(adapted_test_speaker_dir)\n",
    "c = 0\n",
    "for test_folder in sorted(os.listdir(test_dir_path)):\n",
    "    # print(test_folder)\n",
    "    t = []\n",
    "    for test_file in os.listdir(os.path.join(test_dir_path,test_folder)):\n",
    "        t.append(test_file)\n",
    "    fileselected = t[randrange(2)]\n",
    "\n",
    "    \n",
    "    speakerpath = os.path.join(test_dir_path,test_folder,fileselected)\n",
    "    o_filename = \"{}.pkl\".format(test_folder)\n",
    "\n",
    "    \n",
    "    pickle.dump(MAP_Adaptation(speakerpath),open(os.path.join(adapted_test_speaker_dir,o_filename),'wb'))    \n",
    "    c+=1\n",
    "    print('itr :',c)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def supervector(speaker_data_mean):\n",
    "    \"\"\" 'K' Mixtures in the GMM of Dimension 'd' --> Supervector Dimension 'Kd'    \"\"\"\n",
    "    return speaker_data_mean.reshape(-1)\n",
    "\n",
    "\n",
    "def cosine_similarity(x,y):\n",
    "    return np.dot(x,y)/(norm(x)*norm(y))\n",
    "\n",
    "def Eucledian_distance(x,y):\n",
    "    return np.sqrt(np.sum(np.square(x-y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_dir = 'speaker_adapted_ubm_test_1024_zmuv'\n",
    "train_set_dir = 'speaker_adapted_ubm_train_1024_zmuv'\n",
    "pred_labels = []\n",
    "for test_speakers in sorted(os.listdir(test_set_dir)):\n",
    "    score = []\n",
    "    path_test_speakers = os.path.join(test_set_dir,test_speakers)\n",
    "    test_data = supervector(pickle.load(open(path_test_speakers,'rb'))[0])\n",
    "    train_labels = []\n",
    "    for train_speakers in sorted(os.listdir(train_set_dir)):\n",
    "        path_train_speakers = os.path.join(train_set_dir,train_speakers)\n",
    "        train_data = supervector(pickle.load(open(path_train_speakers,'rb'))[0])\n",
    "        # score.append(cosine_similarity(test_data,train_data))\n",
    "        score.append(Eucledian_distance(test_data,train_data))\n",
    "        \n",
    "        train_labels.append(train_speakers.split('.')[0])\n",
    "    # idx = score.index(max(score))\n",
    "    idx = score.index(min(score))\n",
    "\n",
    "    pred_labels.append(train_labels[idx])    \n",
    "  \n",
    "accuracy = accuracy_score(train_labels, pred_labels)\n",
    "print(accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
